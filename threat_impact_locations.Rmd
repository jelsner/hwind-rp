---
title: "Hurricane threat and impact locations"
output: html_document
editor_options: 
  chunk_output_type: console
---

Task: Create a model that outlines areas of hurricane threats and impacts by date. Model output will be collated with health outcome data at the individual level within the areas for those dates

## The forecast cone of uncertainty as threat/impact location model

Data source: https://www.nhc.noaa.gov/gis/ Cones as KML files start in 2012

## Create a model from the tracks data

## Get and import the IBTraCS data 

Geometry type LINESTRING

https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/
https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/doc/IBTrACS_v04_Technical_Details.pdf

https://www.ncei.noaa.gov/sites/default/files/2021-07/IBTrACS_v04_column_documentation.pdf

Wind speed is in units of knots (nautical mile per hour)
```{r}
L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- sf::st_read(dsn = here::here("data"), 
                         layer = "IBTrACS.NA.list.v04r00.lines") |>
  sf::st_transform(crs = 32616)
```

## Summary statistics

Average radius to maximum wind 1981-2022. Values of USA_RMW (radius to maximum winds) and USA_EYE (eye diameter) are in nautical miles. To convert to kilometers multiply by 1.852.
```{r}
Tracks.sf |>
  sf::st_drop_geometry() |>
  dplyr::filter(year >= 1981 & year <= 2022) |>
  dplyr::summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852)
```

Keep only cyclones (USA_WIND >= 64) between 1981 and 2022
```{r}
Tracks2.sf <- Tracks.sf |>
  dplyr::filter(year >= 1981 & year <= 2022) |>
  dplyr::filter(USA_WIND >= 64) |> # change to 34 for tropical storms and hurricanes
  dplyr::select(SID, SEASON, year, month, day, hour, min,
                NAME, SUBBASIN, ISO_TIME,
                USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)
```

Average radius to maximum wind 1981-2022. Values of USA_RMW (radius to maximum winds) and USA_EYE (eye diameter) are in nautical miles. ROCI: Radius of outer closed isobar. To convert to kilometers multiply by 1.852.
```{r}
Tracks2.sf |>
  sf::st_drop_geometry() |>
#  dplyr::group_by(USA_PRES) |>
  dplyr::summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)
```

## Fill in missing RMW values

Start with pedigree, then use minimum pressure, and finish again with pedigree
```{r}
Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |>  # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(USA_PRES) |> # minimum pressure
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |> # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))
```

## Add a buffer to the tracks to make segmented swaths

```{r}
Swaths.sf <- Tracks2.sf |>
  sf::st_buffer(dist = Tracks2.sf$USA_RMW * 1852) # 1852 converts to meters

SingleSwaths.sf <- Swaths.sf |>
  dplyr::group_by(SID) |>
  sf::st_combine()
```

## Florida hurricane swaths

https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html

`USAboundaries` package no longer maintained on CRAN

```{r}
devtools::install_github("ropensci/USAboundariesData")
devtools::install_github("ropensci/USAboundaries")
```

Keep only swaths intersecting the boundary of Florida
```{r}
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersects(Boundaries.sf, sparse = FALSE)

Swaths.sf <- Swaths.sf[X, ]

Swaths.sf <- Swaths.sf |>
  dplyr::mutate(Date = lubridate::as_date(ISO_TIME)) # add a m-y-d column

library(tmap)

tmap::tmap_mode("view")

tmap::tm_shape(Swaths.sf$geometry) +
  tmap::tm_borders()
```

## Extract the boundaries of storm impacts and add threat and post-storm dates

Unionize the swath segments by storm ID
```{r}
Swaths2.sf <- Swaths.sf |>
  dplyr::group_by(SID) |>
  dplyr::summarize(Date0 = dplyr::first(Date),
                   NAME = dplyr::first(NAME),
                   geometry = sf::st_union(geometry))

tmap::tm_shape(Swaths2.sf$geometry) +
  tmap::tm_borders()
```

Expand the data set by adding features based on the increment value of the attribute `Date0`. Earlier dates are threat days
```{r}
deltaT <- 1   # One day increment
n_new <- 3    # Number of new features (rows) to create for each feature (row) times 2

library(tidyr)

# Expand dataset by adding incremented values
expanded_Swaths.sf <- Swaths2.sf |>
    dplyr::rowwise() |>
    dplyr::mutate(new_features = list(tibble(
                  Dates = Date0 + (-n_new:n_new) * deltaT,
                  sid = SID
                  ))) |>
    unnest(new_features) |>
    dplyr::ungroup() |>
    dplyr::select(-Date0, -SID) # Remove original columns
```

Transform the coordinates to lat/lon. Relabel as TIC.sf

```{r}
expanded_Swaths.sf <- expanded_Swaths.sf |>
  sf::st_transform(crs = 4326)

Swaths2.sf <- Swaths2.sf |>
    sf::st_transform(crs = 4326)

TIC.sf <- expanded_Swaths.sf
```

## Merge with health outcome data (deaths)

File received from Jihoon on December 7, 2024 via email. From email:

1) Whether impacted or not when a hurricane passes on the date of death
? What was impacted?
2) Whether impacted or not during a warning on the date of death
3) Whether impacted or not during a watch on the date of death
4) Whether impacted or not during an advisory on the date of death
5) (If available) Whether impacted or not during an evacuation order on the date of death

```{r}
HO.df <- read.csv(file = "data/all_deaths_hurricane.csv")
```

6.2 million deaths

Time period is 1981-1-1 until 2022-12-31. Only dates are given not times. Note: Watch/warning data are only available starting in 2008

lat/lon ranges appear to include Texas through Florida

Initial thoughts: Add a column of type logical to these records that indicate whether a date from the swaths matches AND whether the location falls within the union of the swaths. First need to make this data frame a simple feature data frame with POINT geometry.

```{r}
HO.sf <- HO.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_DEATH)) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(Death_ID = ID, Death_Date = Date)
```

Determine the dates in `HO.sf` that correspond with the dates in `TIC.sf`

```{r}
X <- HO.sf$Death_Date %in% TIC.sf$Dates
sum(X)
Y <- !HO.sf$Death_Date %in% TIC.sf$Dates
sum(Y)
```

68424 death dates correspond to hurricane threat/impact/cleanup dates in Florida. But not all occur within the threat/impact zone
233211 death dates correspond to TS/hurricane threat/impact/cleanup dates in Florida. But not all occur within the threat/impact zone

6097671 death dates correspond to dates outside hurricane TIC dates in Florida
5932884 death dates correspond to dates outside TS/hurricane TIC dates in Florida

Determine the deaths occurring within the threat/impact/cleanup zones

Keep only deaths occurring during hurricane threat/impact/cleanup days in Florida
```{r}
HO2.sf <- HO.sf[X,]
```

Keep only deaths occurring on days outside TIC dates
```{r}
HO3.sf <- HO.sf[Y,]
```

Keep only deaths occurring within the threat/impact/cleanup areas (both during and outside TIC days). First unionize the swath polygons
```{r}
union_Swaths.sf <- Swaths2.sf |>
  sf::st_union()

X <- HO2.sf |>
  sf::st_within(union_Swaths.sf, sparse = FALSE)

HO2.sf <- HO2.sf[X,]

Y <- HO3.sf |>
  sf::st_within(union_Swaths.sf, sparse = FALSE)

HO3.sf <- HO3.sf[Y,]
```

```{r}
nrow(HO2.sf)
length(unique(HO2.sf$Death_Date))
nrow(HO3.sf)
length(unique(HO3.sf$Death_Date))
```

47386 deaths occurred within the impacted areas on the 168 TIC days
4184405 deaths occurred within the impacted areas on the 15172 non TIC days

47836/168 = 285 deaths per day
4184405/15172 = 276 deaths per day

233204 deaths occurred within the impacted areas on 566 TIC days
5932456 deaths occurred within the impacted areas on the 14774 non TIC days

233204/566 = 412
5932456/14774 = 402

Approximately 9 (hur only) and 10 (TS & hur) extra deaths per day during threat/impact/cleanup (7 days). What is the statistical significance of this difference? One way to answer this is to choose random dates and repeat to get a new difference

## Statistical significance of this difference

Define the range of dates. Generate a sequence of all dates in the range commensurate with the range of death dates. Remove dates not occurring during the hurricane season (June - November)
```{r}
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2022-12-31")

all_dates <- seq.Date(from = start_date,
                      to = end_date,
                      by = "day")
months <- lubridate::month(all_dates)
hur_months <- months %in% seq(6, 11)
all_dates <- all_dates[hur_months]
```

```{r}
begin_Time <- Sys.time()
#excess_Deaths <- NULL
#set.seed(896912)  # for reproducibility
for(k in 1:70){
random_dates <- sample(all_dates, size = 24, replace = FALSE) # size here is the number of cyclones
expanded_random_dates <- do.call(c, lapply(random_dates, function(date) seq(date - n_new, date + n_new, by = "day")))
expanded_random_dates <- sort(expanded_random_dates)

X <- HO.sf$Death_Date %in% expanded_random_dates
Y <- !HO.sf$Death_Date %in% expanded_random_dates

HO2.sf <- HO.sf[X,]
HO3.sf <- HO.sf[Y,]

X <- HO2.sf |>
  sf::st_within(union_Swaths.sf, sparse = FALSE)
HO2.sf <- HO2.sf[X,]
Y <- HO3.sf |>
  sf::st_within(union_Swaths.sf, sparse = FALSE)
HO3.sf <- HO3.sf[Y,]

nrow(HO2.sf) / length(unique(HO2.sf$Death_Date))
nrow(HO3.sf) / length(unique(HO3.sf$Death_Date))

difference <- nrow(HO2.sf) / length(unique(HO2.sf$Death_Date)) -
                  nrow(HO3.sf) / length(unique(HO3.sf$Death_Date))
print(difference)

excess_Deaths <- c(difference, excess_Deaths)
}

Sys.time() - begin_Time
```

## Write out the results to a file

For each death assign the outcome to the appropriate date and hurricane
```{r}
indicator <- rep(-3:3, times = 24)
TIC.sf$indicator <- indicator

TIC <- NULL
Hurricane <- NULL
for(i in 1:nrow(HO2.sf)){
TIC <- c(TIC, TIC.sf$indicator[TIC.sf$Dates == HO2.sf$Death_Date[i]])
Hurricane <- c(Hurricane, TIC.sf$NAME[TIC.sf$Dates == HO2.sf$Death_Date[i]])
}
HO2.sf$TIC <- TIC
HO2.sf$Hurricane <- Hurricane
```

The row names of HO2.sf corresponding to the row numbers in the original CSV file

Convert the simple feature data frame to a regular data frame and write to a CSV file

```{r}
coords <- sf::st_coordinates(HO2.sf)

# Combine attributes with coordinates
HO2.df <- HO2.sf |>
    sf::st_drop_geometry() |>  # Drop the geometry column
    cbind(coords) |>           # Add the coordinates as columns
    dplyr::rename(final_lat = Y, final_lon = X)


Output.df <- HO2.df |>
  dplyr::select(Death_ID, Death_Date, final_lat, final_lon, Hurricane, TIC)

write.csv(Output.df, file = "hurDeaths.csv")
```

## Death rates per 100,000 people

To extract the population for an area in the United States defined by a polygon, you can use geospatial data and census data

Install and load required packages

```{r}
install.packages(c("sf", "tidycensus", "tigris", "dplyr"))

library(sf)
library(tidycensus)
library(tigris)
library(dplyr)
```

Set up the Census API

Register for a [Census API key](https://api.census.gov/data/key_signup.html), and load it into your R session:

```{r}
census_api_key("API KEY", install = TRUE)  # already exits
```

Obtain the polygon for your area

Load your polygon (e.g., a shapefile or GeoJSON file)

```{r}
polygon <- union_Swaths.sf
```

Ensure the polygon is in the same coordinate reference system (CRS) as the census data

```{r}
polygon <- sf::st_transform(polygon, crs = 4326)  # Use WGS84 (EPSG:4326)
```

Retrieve census data

Use the `tidycensus` package to retrieve population data for the relevant census geography, such as tracts or block groups

```{r}
state_data <- get_decennial(
  geography = "tract",
  variables = "P1_001N",  # Total population (from Decennial Census)
  state = "FL",           # Replace with your state abbreviation
  geometry = TRUE         # Request spatial data
)
```

Filter census data by the polygon

Perform a spatial join to filter census tracts or block groups that intersect your polygon

```{r}
# Ensure census data and polygon use the same CRS
state_data <- st_transform(state_data, crs = st_crs(polygon))

# Perform spatial intersection
intersected_data <- st_intersection(state_data, polygon)
```

Aggregate the population

Sum the population for the intersected areas

```{r}
# Sum the population variable (e.g., "value" column)
total_population <- intersected_data |>
  summarise(total_population = sum(value, na.rm = TRUE))

print(total_population)
```

15.9 million people out of approximately 23 million

Notes
**Accuracy**: Population data is often tied to census geographies (e.g., tracts, block groups, or blocks). This means populations are aggregated, and exact numbers within custom polygons may require interpolation or more granular data.
**Adjust for Partial Intersections**: If your polygon only partially overlaps some census geographies, you might need to apportion populations based on the area of overlap using `st_area()` or similar functions

47,386 deaths occurred within the impacted areas on the 168 TIC days  (7 days x 24 hurricanes)
47386/15947828 = .002971
.002971/168 = .00001768 deaths/day or 1.768 deaths per 100,000 people per day

4,184,405 deaths occurred within the impacted areas on the 15172 non TIC days
4184405/15947828 = .2623809
.2623809/15172 = .00001729 deaths/day or 1.729 deaths per 100,000 people per day

The annual death rate in Florida is 830 per 100,000 or 830/365 = 2.274 deaths per 100,000 people per day

## Watch/warning areas

Task: Use watch/warning areas. These appear to be only severe local storms (wind, hail, tornadoes)
```{r}
WW.df <- readr::read_csv("data/filtered_hurricane_data_FL.csv")

WW.sf <- sf::st_read(dsn = here::here("data"), 
                     layer = "z_05mr24")
```

## Merge with health outcome data (births)

Email from Jihoon on December 11, 2024

1. ID 
2. DATE_OF_BIRTH 
3. final_lat  
4. final_lon 
5. trimester1_s_date: The first Trimester start date
6. trimester1_e_date : The first Trimester end date
7. trimester2_s_date : The second Trimester start date
8. trimester2_e_date : The second Trimester end date
9. trimester3_s_date: The third Trimester start date
# The first Trimester: the weeks 1-13 
# The second Trimester: the weeks 14-28
# The third Trimester: the weeks 29 ~

```{r}
Births.df <- read.csv(file = "data/all_births_hurricane.csv")
```

7.2 million births. Time period is 1981-1-1 until 2022-12-31. Only dates are given not times. lat/lon ranges appear to include Texas through Florida

Make the data frame a simple feature data frame with POINT geometry

```{r}

Births.sf <- Births.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_BIRTH),
                DateT1s = lubridate::as_date(trimester1_s_date),
                DateT1e = lubridate::as_date(trimester1_e_date),
                DateT2s = lubridate::as_date(trimester2_s_date),
                DateT2e = lubridate::as_date(trimester2_e_date),
                DateT3s = lubridate::as_date(trimester3_s_date)
                ) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(Birth_ID = ID, Birth_Date = Date, DateT1s, DateT1e, DateT2s, DateT2e, DateT3s)
```

Determine whether there was a hurricane during each of the trimesters

```{r}
X <- HO.sf$Death_Date %in% TIC.sf$Dates
sum(X)
Y <- !HO.sf$Death_Date %in% TIC.sf$Dates
sum(Y)
```


## Scratch

I have two simple feature data frames. One contains a date column and a simple feature column of POINT geometry the other contains a date column and a simple feature column of POLYGON geometry. For the first data frame I want to add a column indicating whether the date matches a date in the second data frame AND if the corresponding POINT lies within the corresponding POLYGON.

```{r}
library(sf)
library(dplyr)

# Example POINTS sf data frame
points_sf <- st_sf(
    id = 1:3,
    date = as.Date(c("2024-01-01", "2024-01-02", "2024-01-03")),
    geometry = st_sfc(
        st_point(c(1, 1)),
        st_point(c(2, 2)),
        st_point(c(3, 3))
    )
)

# Example POLYGONS sf data frame
polygons_sf <- st_sf(
    id = 1:2,
    date = as.Date(c("2024-01-01", "2024-01-02")),
    geometry = st_sfc(
        st_polygon(list(rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)))),
        st_polygon(list(rbind(c(1, 1), c(3, 1), c(3, 3), c(1, 3), c(1, 1))))
    )
)

# Perform the Spatial Join Use a combination of date matching and spatial intersection checks

# Add the matching column
points_with_matches <- points_sf %>%
    rowwise() %>%  # Process each POINT individually
    mutate(
        match = any(
            (date == polygons_sf$date) &  # Check date match
            st_within(geometry, polygons_sf$geometry) %>% as.logical()  # Check if POINT lies in POLYGON
        )
    ) %>%
    ungroup()

```

```{r}
# Add the matching column
points_with_matches <- health2.sf |>
    dplyr::rowwise() |>  # Process each POINT individually
    dplyr::mutate(match = any((Date == expanded_Swaths.sf$Dates[1]) &  # Check date match
            sf::st_within(geometry, expanded_Swaths.sf$geometry[1]) |>
              as.logical()  # Check if POINT lies in POLYGON
        )
    ) |>
    dplyr::ungroup()
```


## Extract the counties affected by the storms

```{r}
Boundaries.sf <- USAboundaries::us_counties(resolution = "low", states = "FL") |> 
  sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersection(y = Boundaries.sf) |>
  dplyr::select(geoid, name, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME)) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(geoid, date) |>
  dplyr::summarize(CountyName = dplyr::first(name),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES))

X |>
  dplyr::group_by(CountyName) |>
  dplyr::summarize(nTC = dplyr::n(),
                   Wmax = max(Wind_kt, na.rm = TRUE),
                   Pmin = min(Press_mb, na.rm = TRUE)) |>
  dplyr::arrange(desc(nTC))
```

## Repeat for census tracts

```{r}
Boundaries.sf <- tidycensus::get_acs(
  geography = "tract",
  variables = "B01003_001",
  state = "FL",
  geometry = TRUE
  ) |>
    sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersection(y = Boundaries.sf) |>
  dplyr::select(GEOID, NAME.1, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME),
                StormNameYear = paste0(NAME, "-", lubridate::year(date))) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(GEOID, date) |>
  dplyr::summarize(AreaName = dplyr::first(NAME.1),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES),
                   StormNameYear = dplyr::first(StormNameYear)) |>
  dplyr::arrange(date, GEOID) |>
  dplyr::left_join(Boundaries.sf, by = "GEOID", keep = FALSE) |>
  sf::st_as_sf()

#write.csv(X, here::here("data", "place-level-TC-winds-pressure"))
```

What storm impacted the most areas?
```{r}
X |> 
  dplyr::group_by(StormNameYear) |>
  dplyr::summarize(nAreasImpacted = dplyr::n()) |>
  dplyr::arrange(desc(nAreasImpacted))
```

## Repeat for city areas (zip code tabulation area, ztca)

```{r}
library(tidycensus)

# valid years are 2000, 2010, and 2020
Boundaries.sf <- get_decennial(
  geography = "zcta",
  variables = "P005003",
  year = 2010,
  state = "FL",  # one state at a time
  geometry = TRUE
  ) |>
  sf::st_transform(crs = 32616)

Boundaries.sf <- get_acs(
  geography = "place",  # cities/metro areas
  variables = "B01003_001",
  year = 2020, #years 2014-2018
  state = c("FL"),
  geometry = TRUE) |>
  sf::st_transform(crs = 32616)


X <- Swaths.sf |>
  sf::st_intersection(y = Boundaries2.sf) |>
  dplyr::select(GEOID, NAME.1, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME),
                StormNameYear = paste0(NAME, "-", lubridate::year(date))) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(date, GEOID) |>
  dplyr::summarize(CityName = dplyr::first(NAME.1),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES),
                   StormNameYear = dplyr::first(StormNameYear)) |>
  dplyr::arrange(date, desc(Wind_kt))
```


## Get PRISM rainfall data for Hurricane Kate (1985)

```{r}
library(prism)

prism_set_dl_dir("data/PRISM")

get_prism_dailys(type = "ppt",
                 minDate = "1985-11-21",
                 maxDate = "1985-11-22")

dates <- seq(lubridate::as_date("1985-11-21"), 
             lubridate::as_date("1985-11-22"), 
             by = "day")

file_days <- gsub("-", "", dates)

folder_names <- paste0("PRISM_ppt_stable_4kmD2_", file_days, "_bil/")
file_names <- paste0("PRISM_ppt_stable_4kmD2_", file_days, "_bil.bil")
file_list <- paste0("data/", "PRISM/", folder_names, file_names)

ppt.st <- stars::read_stars(file_list, along = list(time = dates)) |>
  setNames("Precip_mm")

plot(ppt.st)
```

Extract rainfall from space-time `stars` object at a particular target location (e.g., centroid of zip code area).
```{r}
print(ppt.st)

library(sf)
st_crs(ppt.st)

target_location <- st_sfc(st_point(c(-84.28, 30.43)), crs = 4326) 
target_location_transformed <- st_transform(target_location, 
                                            st_crs(ppt.st))

time_series_data <- stars::st_extract(ppt.st, target_location_transformed)
time_series_data$Precip_mm
```

Crop prism rasters to census tracts affected by Kate

```{r}

X2 <- X |>
  dplyr::filter(StormNameYear == "KATE-1985") |>
  sf::st_transform(crs = sf::st_crs(ppt.st))

plot(X2$geometry)

ppt2.st <- ppt.st |>
  sf::st_crop(X2)

plot(ppt2.st)

#must make ppt.st a raster of some type

library(raster)
ppt2.r <- as.raster(ppt2.st)

exactextractr::exact_extract(ppt.st, X2, fun = max)
```




## Repeat for city areas

```{r}
library(tidycensus)

Boundaries2.sf <- get_decennial(geography = "zcta",
                                variables = "B03002_001",
                                year = 2010,
                                geometry = TRUE) |>
  sf::st_transform(crs = 32616)

Boundaries2.sf <- get_acs(geography = "zip code tabulation area", 
                          variables = "B19013_001",
                          summary_var = "B01001_001",
                          geometry = TRUE)


Boundaries2.sf <- get_acs(geography = "place",
                      variables = "B01003_001",
                      state = c("FL", "AL", "MS", "LA", "TX"),
                      geometry = TRUE) |>
    sf::st_transform(crs = 32616)


X <- Tracks.sf |>
  dplyr::filter(year >= 1980) |>
  sf::st_intersection(y = Boundaries2.sf) |>
  dplyr::select(GEOID, NAME.1, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME),
                StormNameYear = paste0(NAME, "-", lubridate::year(date))) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(date, GEOID) |>
  dplyr::summarize(CityName = dplyr::first(NAME.1),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES),
                   StormNameYear = dplyr::first(StormNameYear)) |>
  dplyr::arrange(date, desc(Wind_kt))
```


## Find all storms within 100 km of a location

```{r}
point <- sf::st_point(c(-85.3, 29.9))
location.sf <- sf::st_sfc(point, 
                           crs = 4326) |>
  sf::st_transform(crs = 32616) |>
  sf::st_as_sf()

buffer.sf <- location.sf |>
  sf::st_buffer(dist = 100000)

Tracks2.sf <- Tracks.sf |>
  sf::st_intersection(buffer.sf)

Tracks3.sf <- Tracks2.sf[!is.na(Tracks2.sf$USA_ATCFID),]

length(unique(Tracks3.sf$USA_ATCFID))
max(Tracks3.sf$SEASON) - min(Tracks3.sf$SEASON) + 1
```

List the unique storms by `USA_ATCFID` = `track_id`
```{r}
unique(Tracks3.sf$USA_ATCFID)
```

Group by storm ID and find the fastest wind
```{r}
X <- Tracks3.sf |>
  sf::st_drop_geometry() |>
  dplyr::group_by(USA_ATCFID) |>
  dplyr::summarize(Wmax = max(WMO_WIND, na.rm = TRUE))
```

```{r}
tmap::tmap_mode("view")
tmap::tm_shape(Tracks2.sf) +
  tmap::tm_lines()
```

## Wind field model

<https://github.com/jbcannon/hurrecon/tree/v.0.0.1>

```{r}
devtools::install_github('jbcannon/hurrecon')

library(hurrecon)
path <- here::here("data", "hurdat_data.csv")
fetch_best_tracks_data(path,
                       src = 
"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2023-051124.txt")
```

The last 6 numbers before the `.txt` are the date of the archive. As of 6 June 2024, the archive date is 5 May 2023

Load the data for Hurricane Michael (2018, ID = AL142018) as a simple feature data frame with CRS UTM16 (EPSG 32616)
```{r}
Michael_Track.sf <- load_hurdat_track(path, 
                                      trackID = 'AL142018')
X.sf <- load_hurdat_track(path,
                          trackID = 'AL041851')
```

Get land mask
```{r}
library(terra)
data("geographic")
```

Create raster of winds with units of m/s. This takes a long time (13 minutes on desktop)
```{r}
t0 <- Sys.time()
Wind.r <- hurrecon_run(Michael_Track.sf, 
                       max_rad_km = 100, 
                       res_m = 30000, 
                       max_interp_dist_km = 5)
Sys.time() - t0
```

Make a plot
```{r}
plot(land)
plot(Wind.r, add = TRUE)
plot(land, add = TRUE)
```

Extract the wind speed from the raster at a particular location.

Create the location
```{r}
point <- sf::st_point(c(-85.3, 29.9))
location.sf <- sf::st_sfc(point, 
                           crs = 4326) |>
  sf::st_transform(crs = sf::st_crs(Wind.r)) |>
  sf::st_as_sf()

Wind.r |>
  terra::extract(location.sf)
```


Include only track observations when the track is within 200 km of a location
```{r}
buffer.sf <- location.sf |>
  sf::st_buffer(dist = 200000)

Michael_Track2.sf <- Michael_Track.sf |>
  sf::st_intersection(buffer.sf) 

colnames(Michael_Track2.sf)[11:22] <- colnames(Michael_Track.sf)[11:22]
```

Run the wind field model
```{r}
t0 <- Sys.time()
Wind.r2 <- hurrecon_run(Michael_Track2.sf, 
                        max_rad_km = 100, res_m = 5000, max_interp_dist_km = 5)
Sys.time() - t0
```

32 seconds

```{r}
tmap::tmap_mode("view")
tmap::tm_shape(Wind.r2) +
  tmap::tm_raster(alpha = .5) +
  tmap::tm_shape(Michael_Track.sf) +
    tmap::tm_dots()
```

Multiple storms wind fields for a specific location into a single `SpatVector`.

Get all tracks
```{r}
df <- read.csv(here::here("data", "hurdat_data.csv"))
df$date <- lubridate::ymd(df$date)
df$lon2 <- as.numeric(df$lon)
df$lat2 <- as.numeric(df$lat)
df <- df[complete.cases(df[, c("lon2")]), ]

sdf <- df |>
  sf::st_as_sf(coords = c("lon2", "lat2"))
sf::st_crs(sdf) <- 4326

sdf <- sdf |>
  sf::st_transform(crs = 32616)

sdf[, c("utmx", "utmy")] <- sf::st_coordinates(sdf)

cn <- c("34kt_ne", "34kt_se",	"34kt_sw", "34kt_nw",	
        "50kt_ne", "50kt_se",	"50kt_sw", "50kt_nw",
        "64kt_ne", "64kt_se",	"64kt_sw", "64kt_nw")
colnames(sdf)[11:22] <- cn
```