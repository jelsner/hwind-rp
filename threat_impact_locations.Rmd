---
title: "Hurricane threat and impact locations"
output: html_document
editor_options: 
  chunk_output_type: console
---

Task: Create a model that outlines areas of hurricane threats and impacts by date. Model output will be collated with health outcome data at the individual level within the areas for those dates.

## The forecast cone of uncertainty as threat/impact location model

Data source: https://www.nhc.noaa.gov/gis/ Cones as KML files start in 2012

## Create a model from the tracks data

## Get and import the IBTraCS data 

Geometry type LINESTRING

https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/
https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/doc/IBTrACS_v04_Technical_Details.pdf

https://www.ncei.noaa.gov/sites/default/files/2021-07/IBTrACS_v04_column_documentation.pdf

Wind speed is in units of knots (nautical mile per hour)
```{r}
L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- sf::st_read(dsn = here::here("data"), 
                         layer = "IBTrACS.NA.list.v04r00.lines") |>
  sf::st_transform(crs = 32616)
```

## Summary statistics

Average radius to maximum wind 1981-2022. Values of USA_RMW (radius to maximum winds) and USA_EYE (eye diameter) are in nautical miles. To convert to kilometers multiply by 1.852.
```{r}
Tracks.sf |>
  sf::st_drop_geometry() |>
  dplyr::filter(year >= 1981 & year <= 2022) |>
  dplyr::summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852)
```

Keep only hurricanes (USA_WIND >= 64) between 1981 and 2022
```{r}
Tracks2.sf <- Tracks.sf |>
  dplyr::filter(year >= 1981 & year <= 2022) |>
  dplyr::filter(USA_WIND >= 64) |>
  dplyr::select(SID, SEASON, year, month, day, hour, min,
                NAME, SUBBASIN, ISO_TIME,
                USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)
```

Average radius to maximum wind 1981-2022. Values of USA_RMW (radius to maximum winds) and USA_EYE (eye diameter) are in nautical miles. ROCI: Radius of outer closed isobar. To convert to kilometers multiply by 1.852.
```{r}
Tracks2.sf |>
  sf::st_drop_geometry() |>
#  dplyr::group_by(USA_PRES) |>
  dplyr::summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)
```

## Fill in missing RMW values

Start with pedigree, then use minimum pressure, and finish again with pedigree
```{r}
Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |>  # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(USA_PRES) |> # minimum pressure
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |> # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))
```

## Add a buffer to the tracks to make segmented swaths

```{r}
Swaths.sf <- Tracks2.sf |>
  sf::st_buffer(dist = Tracks2.sf$USA_RMW * 1852) # 1852 converts to meters

SingleSwaths.sf <- Swaths.sf |>
  dplyr::group_by(SID) |>
  sf::st_combine()

library(tmap)

tmap::tmap_mode("view")

tmap::tm_shape(Swaths.sf) +
  tmap::tm_borders()
```

## Florida hurricane swaths

https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html

`USAboundaries` package no longer maintained on CRAN

```{r}
devtools::install_github("ropensci/USAboundariesData")
devtools::install_github("ropensci/USAboundaries")
```

Keep only swaths intersecting the boundary of Florida
```{r}
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersects(Boundaries.sf, sparse = FALSE)

Swaths.sf <- Swaths.sf[X, ]

Swaths.sf <- Swaths.sf |>
  dplyr::mutate(Date = lubridate::as_date(ISO_TIME)) # add a m-y-d column
```

## Extract the boundaries of storm impacts and add threat and post-storm dates

Unionize the swath segments by storm ID
```{r}
Swaths2.sf <- Swaths.sf |>
  dplyr::group_by(SID) |>
  dplyr::summarize(Date0 = dplyr::first(Date),
                   NAME = dplyr::first(NAME),
                   geometry = sf::st_union(geometry))
```

Expand the data set by adding features based on the increment value of the attribute `Date0`. Earlier dates are threat days
```{r}
deltaT <- 1   # One day increment
n_new <- 3    # Number of new features (rows) to create for each feature (row) times 2

library(tidyr)

# Expand dataset by adding incremented values
expanded_Swaths.sf <- Swaths2.sf |>
    dplyr::rowwise() |>
    dplyr::mutate(new_features = list(tibble(
                  Dates = Date0 + (-n_new:n_new) * deltaT,
                  sid = SID
                  ))) |>
    unnest(new_features) |>
    dplyr::ungroup() |>
    dplyr::select(-Date0, -SID) # Remove original columns
```

Transform the coordinates to lat/lon. Relabel as TIC.sf

```{r}
expanded_Swaths.sf <- expanded_Swaths.sf |>
  sf::st_transform(crs = 4326)

Swaths2.sf <- Swaths2.sf |>
    sf::st_transform(crs = 4326)

TIC.sf <- expanded_Swaths.sf
```

## Health outcome data

File received from Jihoon on December 7, 2024 via email. From email:

1) Whether impacted or not when a hurricane passes on the date of death
? What was impacted?
2) Whether impacted or not during a warning on the date of death
3) Whether impacted or not during a watch on the date of death
4) Whether impacted or not during an advisory on the date of death
5) (If available) Whether impacted or not during an evacuation order on the date of death

```{r}
HO.df <- read.csv(file = "data/all_deaths_hurricane.csv")
```

6.2 million deaths

Time period is 1981-1-1 until 2022-12-31. Only dates are given not times. Note: Watch/warning data are only available starting in 2008

lat/lon ranges appear to include Texas through Florida

Initial thoughts: Add a column of type logical to these records that indicate whether a date from the swaths matches AND whether the location falls within the union of the swaths. First need to make this data frame a simple feature data frame with POINT geometry.

```{r}
HO.sf <- HO.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_DEATH)) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(Death_ID = ID, Death_Date = Date)
```

## Determine the dates in `HO.sf` that correspond with the dates in `TIC.sf`

```{r}
X <- HO.sf$Death_Date %in% TIC.sf$Dates
sum(X)
```

68424 death dates correspond to hurricane threat/impact/cleanup dates in Florida. But not all occur within the threat/impact zone

## Determine the deaths occurring within the threat/impact/cleanup zones

Filter out deaths not occurring on hurricane threat/impact/cleanup days in Florida
```{r}
HO2.sf <- HO.sf[X,]
```

Filter out deaths not occurring within the threat/impact/cleanup areas
```{r}
union_Swaths.sf <- Swaths2.sf |>
  sf::st_union()

X <- HO2.sf |>
  sf::st_within(union_Swaths.sf, sparse = FALSE)

HO2.sf <- HO2.sf[X,]
```

47386 death dates corresponding to TIC dates and occurring within the impact areas

For each death assign the outcome to the appropriate date and hurricane
```{r}
indicator <- rep(-3:3, times = 24)
TIC.sf$indicator <- indicator

TIC <- NULL
Hurricane <- NULL
for(i in 1:nrow(HO2.sf)){
TIC <- c(TIC, TIC.sf$indicator[TIC.sf$Dates == HO2.sf$Death_Date[i]])
Hurricane <- c(Hurricane, TIC.sf$NAME[TIC.sf$Dates == HO2.sf$Death_Date[i]])
}
HO2.sf$TIC <- TIC
HO2.sf$Hurricane <- Hurricane
```

The row names of HO2.sf corresponding to the row numbers in the original CSV file

Convert the simple feature data frame to a regular data frame and write to a CSV file

```{r}
coords <- sf::st_coordinates(HO2.sf)

# Combine attributes with coordinates
HO2.df <- HO2.sf |>
    sf::st_drop_geometry() |>  # Drop the geometry column
    cbind(coords) |>           # Add the coordinates as columns
    dplyr::rename(final_lat = Y, final_lon = X)


Output.df <- HO2.df |>
  dplyr::select(Death_ID, Death_Date, final_lat, final_lon, Hurricane, TIC)

write.csv(Output.df, file = "hurDeaths.csv")
```


# Scratch

I have two simple feature data frames. One contains a date column and a simple feature column of POINT geometry the other contains a date column and a simple feature column of POLYGON geometry. For the first data frame I want to add a column indicating whether the date matches a date in the second data frame AND if the corresponding POINT lies within the corresponding POLYGON.

```{r}
library(sf)
library(dplyr)

# Example POINTS sf data frame
points_sf <- st_sf(
    id = 1:3,
    date = as.Date(c("2024-01-01", "2024-01-02", "2024-01-03")),
    geometry = st_sfc(
        st_point(c(1, 1)),
        st_point(c(2, 2)),
        st_point(c(3, 3))
    )
)

# Example POLYGONS sf data frame
polygons_sf <- st_sf(
    id = 1:2,
    date = as.Date(c("2024-01-01", "2024-01-02")),
    geometry = st_sfc(
        st_polygon(list(rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)))),
        st_polygon(list(rbind(c(1, 1), c(3, 1), c(3, 3), c(1, 3), c(1, 1))))
    )
)

# Perform the Spatial Join Use a combination of date matching and spatial intersection checks

# Add the matching column
points_with_matches <- points_sf %>%
    rowwise() %>%  # Process each POINT individually
    mutate(
        match = any(
            (date == polygons_sf$date) &  # Check date match
            st_within(geometry, polygons_sf$geometry) %>% as.logical()  # Check if POINT lies in POLYGON
        )
    ) %>%
    ungroup()

```




```{r}
# Add the matching column
points_with_matches <- health2.sf |>
    dplyr::rowwise() |>  # Process each POINT individually
    dplyr::mutate(match = any((Date == expanded_Swaths.sf$Dates[1]) &  # Check date match
            sf::st_within(geometry, expanded_Swaths.sf$geometry[1]) |>
              as.logical()  # Check if POINT lies in POLYGON
        )
    ) |>
    dplyr::ungroup()
```



## Extract the counties affected by the storms

```{r}
Boundaries.sf <- USAboundaries::us_counties(resolution = "low", states = "FL") |> 
  sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersection(y = Boundaries.sf) |>
  dplyr::select(geoid, name, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME)) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(geoid, date) |>
  dplyr::summarize(CountyName = dplyr::first(name),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES))

X |>
  dplyr::group_by(CountyName) |>
  dplyr::summarize(nTC = dplyr::n(),
                   Wmax = max(Wind_kt, na.rm = TRUE),
                   Pmin = min(Press_mb, na.rm = TRUE)) |>
  dplyr::arrange(desc(nTC))
```

## Repeat for census tracts

```{r}
Boundaries.sf <- tidycensus::get_acs(
  geography = "tract",
  variables = "B01003_001",
  state = "FL",
  geometry = TRUE
  ) |>
    sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersection(y = Boundaries.sf) |>
  dplyr::select(GEOID, NAME.1, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME),
                StormNameYear = paste0(NAME, "-", lubridate::year(date))) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(GEOID, date) |>
  dplyr::summarize(AreaName = dplyr::first(NAME.1),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES),
                   StormNameYear = dplyr::first(StormNameYear)) |>
  dplyr::arrange(date, GEOID) |>
  dplyr::left_join(Boundaries.sf, by = "GEOID", keep = FALSE) |>
  sf::st_as_sf()

#write.csv(X, here::here("data", "place-level-TC-winds-pressure"))
```

What storm impacted the most areas?
```{r}
X |> 
  dplyr::group_by(StormNameYear) |>
  dplyr::summarize(nAreasImpacted = dplyr::n()) |>
  dplyr::arrange(desc(nAreasImpacted))
```

## Repeat for city areas (zip code tabulation area, ztca)

```{r}
library(tidycensus)

# valid years are 2000, 2010, and 2020
Boundaries.sf <- get_decennial(
  geography = "zcta",
  variables = "P005003",
  year = 2010,
  state = "FL",  # one state at a time
  geometry = TRUE
  ) |>
  sf::st_transform(crs = 32616)

Boundaries.sf <- get_acs(
  geography = "place",  # cities/metro areas
  variables = "B01003_001",
  year = 2020, #years 2014-2018
  state = c("FL"),
  geometry = TRUE) |>
  sf::st_transform(crs = 32616)


X <- Swaths.sf |>
  sf::st_intersection(y = Boundaries2.sf) |>
  dplyr::select(GEOID, NAME.1, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME),
                StormNameYear = paste0(NAME, "-", lubridate::year(date))) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(date, GEOID) |>
  dplyr::summarize(CityName = dplyr::first(NAME.1),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES),
                   StormNameYear = dplyr::first(StormNameYear)) |>
  dplyr::arrange(date, desc(Wind_kt))
```


## Get PRISM rainfall data for Hurricane Kate (1985)

```{r}
library(prism)

prism_set_dl_dir("data/PRISM")

get_prism_dailys(type = "ppt",
                 minDate = "1985-11-21",
                 maxDate = "1985-11-22")

dates <- seq(lubridate::as_date("1985-11-21"), 
             lubridate::as_date("1985-11-22"), 
             by = "day")

file_days <- gsub("-", "", dates)

folder_names <- paste0("PRISM_ppt_stable_4kmD2_", file_days, "_bil/")
file_names <- paste0("PRISM_ppt_stable_4kmD2_", file_days, "_bil.bil")
file_list <- paste0("data/", "PRISM/", folder_names, file_names)

ppt.st <- stars::read_stars(file_list, along = list(time = dates)) |>
  setNames("Precip_mm")

plot(ppt.st)
```

Extract rainfall from space-time `stars` object at a particular target location (e.g., centroid of zip code area).
```{r}
print(ppt.st)

library(sf)
st_crs(ppt.st)

target_location <- st_sfc(st_point(c(-84.28, 30.43)), crs = 4326) 
target_location_transformed <- st_transform(target_location, 
                                            st_crs(ppt.st))

time_series_data <- stars::st_extract(ppt.st, target_location_transformed)
time_series_data$Precip_mm
```

Crop prism rasters to census tracts affected by Kate

```{r}

X2 <- X |>
  dplyr::filter(StormNameYear == "KATE-1985") |>
  sf::st_transform(crs = sf::st_crs(ppt.st))

plot(X2$geometry)

ppt2.st <- ppt.st |>
  sf::st_crop(X2)

plot(ppt2.st)

#must make ppt.st a raster of some type

library(raster)
ppt2.r <- as.raster(ppt2.st)

exactextractr::exact_extract(ppt.st, X2, fun = max)
```




## Repeat for city areas

```{r}
library(tidycensus)

Boundaries2.sf <- get_decennial(geography = "zcta",
                                variables = "B03002_001",
                                year = 2010,
                                geometry = TRUE) |>
  sf::st_transform(crs = 32616)

Boundaries2.sf <- get_acs(geography = "zip code tabulation area", 
                          variables = "B19013_001",
                          summary_var = "B01001_001",
                          geometry = TRUE)


Boundaries2.sf <- get_acs(geography = "place",
                      variables = "B01003_001",
                      state = c("FL", "AL", "MS", "LA", "TX"),
                      geometry = TRUE) |>
    sf::st_transform(crs = 32616)


X <- Tracks.sf |>
  dplyr::filter(year >= 1980) |>
  sf::st_intersection(y = Boundaries2.sf) |>
  dplyr::select(GEOID, NAME.1, ISO_TIME, NAME, USA_WIND, USA_PRES) |>
  dplyr::mutate(date = lubridate::as_date(ISO_TIME),
                StormNameYear = paste0(NAME, "-", lubridate::year(date))) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(date, GEOID) |>
  dplyr::summarize(CityName = dplyr::first(NAME.1),
                   Wind_kt = max(USA_WIND),
                   Press_mb = min(USA_PRES),
                   StormNameYear = dplyr::first(StormNameYear)) |>
  dplyr::arrange(date, desc(Wind_kt))
```


## Find all storms within 100 km of a location

```{r}
point <- sf::st_point(c(-85.3, 29.9))
location.sf <- sf::st_sfc(point, 
                           crs = 4326) |>
  sf::st_transform(crs = 32616) |>
  sf::st_as_sf()

buffer.sf <- location.sf |>
  sf::st_buffer(dist = 100000)

Tracks2.sf <- Tracks.sf |>
  sf::st_intersection(buffer.sf)

Tracks3.sf <- Tracks2.sf[!is.na(Tracks2.sf$USA_ATCFID),]

length(unique(Tracks3.sf$USA_ATCFID))
max(Tracks3.sf$SEASON) - min(Tracks3.sf$SEASON) + 1
```

List the unique storms by `USA_ATCFID` = `track_id`
```{r}
unique(Tracks3.sf$USA_ATCFID)
```

Group by storm ID and find the fastest wind
```{r}
X <- Tracks3.sf |>
  sf::st_drop_geometry() |>
  dplyr::group_by(USA_ATCFID) |>
  dplyr::summarize(Wmax = max(WMO_WIND, na.rm = TRUE))
```

```{r}
tmap::tmap_mode("view")
tmap::tm_shape(Tracks2.sf) +
  tmap::tm_lines()
```

## Wind field model

<https://github.com/jbcannon/hurrecon/tree/v.0.0.1>

```{r}
devtools::install_github('jbcannon/hurrecon')

library(hurrecon)
path <- here::here("data", "hurdat_data.csv")
fetch_best_tracks_data(path,
                       src = 
"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2023-051124.txt")
```

The last 6 numbers before the `.txt` are the date of the archive. As of 6 June 2024, the archive date is 5 May 2023

Load the data for Hurricane Michael (2018, ID = AL142018) as a simple feature data frame with CRS UTM16 (EPSG 32616)
```{r}
Michael_Track.sf <- load_hurdat_track(path, 
                                      trackID = 'AL142018')
X.sf <- load_hurdat_track(path,
                          trackID = 'AL041851')
```

Get land mask
```{r}
library(terra)
data("geographic")
```

Create raster of winds with units of m/s. This takes a long time (13 minutes on desktop)
```{r}
t0 <- Sys.time()
Wind.r <- hurrecon_run(Michael_Track.sf, 
                       max_rad_km = 100, 
                       res_m = 30000, 
                       max_interp_dist_km = 5)
Sys.time() - t0
```

Make a plot
```{r}
plot(land)
plot(Wind.r, add = TRUE)
plot(land, add = TRUE)
```

Extract the wind speed from the raster at a particular location.

Create the location
```{r}
point <- sf::st_point(c(-85.3, 29.9))
location.sf <- sf::st_sfc(point, 
                           crs = 4326) |>
  sf::st_transform(crs = sf::st_crs(Wind.r)) |>
  sf::st_as_sf()

Wind.r |>
  terra::extract(location.sf)
```


Include only track observations when the track is within 200 km of a location
```{r}
buffer.sf <- location.sf |>
  sf::st_buffer(dist = 200000)

Michael_Track2.sf <- Michael_Track.sf |>
  sf::st_intersection(buffer.sf) 

colnames(Michael_Track2.sf)[11:22] <- colnames(Michael_Track.sf)[11:22]
```

Run the wind field model
```{r}
t0 <- Sys.time()
Wind.r2 <- hurrecon_run(Michael_Track2.sf, 
                        max_rad_km = 100, res_m = 5000, max_interp_dist_km = 5)
Sys.time() - t0
```

32 seconds

```{r}
tmap::tmap_mode("view")
tmap::tm_shape(Wind.r2) +
  tmap::tm_raster(alpha = .5) +
  tmap::tm_shape(Michael_Track.sf) +
    tmap::tm_dots()
```

Multiple storms wind fields for a specific location into a single `SpatVector`.

Get all tracks
```{r}
df <- read.csv(here::here("data", "hurdat_data.csv"))
df$date <- lubridate::ymd(df$date)
df$lon2 <- as.numeric(df$lon)
df$lat2 <- as.numeric(df$lat)
df <- df[complete.cases(df[, c("lon2")]), ]

sdf <- df |>
  sf::st_as_sf(coords = c("lon2", "lat2"))
sf::st_crs(sdf) <- 4326

sdf <- sdf |>
  sf::st_transform(crs = 32616)

sdf[, c("utmx", "utmy")] <- sf::st_coordinates(sdf)

cn <- c("34kt_ne", "34kt_se",	"34kt_sw", "34kt_nw",	
        "50kt_ne", "50kt_se",	"50kt_sw", "50kt_nw",
        "64kt_ne", "64kt_se",	"64kt_sw", "64kt_nw")
colnames(sdf)[11:22] <- cn
```